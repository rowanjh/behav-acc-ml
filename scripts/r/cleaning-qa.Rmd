---
title: "Data cleaning quality check"
author: "Rowan Jacques-Hamilton"
date: "`r Sys.Date()`"
output: html_document
---

This notebook checks for bugs in the data cleaning, including domains of: 
* Behaviour label alignment & drift rates 
* Accelerometer data scaling (6-O desk scaling, and on-bird median scaling)

It was mainly for internal use during development and is not as well polished or
commented as the main analysis scripts.

```{r Loading & Prep}
library(here)
library(ggplot2)
library(dplyr)
library(purrr)
library(glue)
library(tidyr)
# library(gridExtra)
library(doParallel)
library(RSQLite)
library(data.table)
library(lubridate)
library(stringi)
library(patchwork)
source(here("scripts", "r", "cleaning-helpers.R"))

# Critical dated output directory: change to latest version
processed_data_dir <- here("data","clean","data-segments")

# Raw files
path_raw_acc_db <- here("data", "raw", "ruff-acc-data.db")
path_raw_acc_db <- "~/ruff-acc.db" #TODO remove this for final version
path_deploy_notes <- here("data", "raw", "logger_deployment_notes.csv")
path_6O_calib_info <- here("data", "raw", "calibration_recordings_6O_Apr2022.csv")
dir_acc_calib_data <- here("data", "raw", "6O_calibration_files")

# Processed files
beh_obs_file <- here("data", "clean", "ruff_behaviours_adjusted_for_drift.csv")
path_recording_info <- here("data", "clean", "recording_info.csv")
segment_summary_file <- here("data", "clean", "segment_summary.csv")

## Parameters
num_cores <- min(20, parallel::detectCores())
options(digits.secs = 4)
acc_sr = 50 # sampling rate, Hz (samples / second), used by some helpers

# Helpers
last <- function(x) {return(x[length(x)])}

## Load beh data
beh_obs_data <- fread(beh_obs_file, data.table = FALSE, tz = "")

# Convert times to useful format, subset to useful columns
beh_obs_data <- beh_obs_data |> 
    mutate(beh_start_acc = toposix_ymdhms(beh_start_acc),
           beh_stop_acc = toposix_ymdhms(beh_stop_acc), 
           beh_start_real = toposix_ymdhms(beh_start_real), 
           beh_stop_real = toposix_ymdhms(beh_stop_real)) |>
  select(recording_id, beh_event_id, behaviour, beh_start_real, beh_stop_real, 
         beh_start_acc, beh_stop_acc, duration_secs)

# Load segment summary: shows segments of continuously scored behaviour
segment_summary <- read.csv(segment_summary_file) |>
    mutate(seg_start_real = toposix_ymdhms(seg_start_real), 
           seg_start_acc = toposix_ymdhms(seg_start_acc), 
           seg_stop_real = toposix_ymdhms(seg_stop_real),
           seg_stop_acc = toposix_ymdhms(seg_stop_acc))

## Load deployment notes
deploy_notes <- read.csv(path_deploy_notes)

## Load recording info
recording_info <- read.csv(path_recording_info)
```
Plots for clock error calculations
```{r Plots for clock error calculations}
# Get bird acc segments at calibration points
# Calculate drift rates from that
for(i in 1:nrow(recording_info)){
    this_recording_info <- recording_info[i,]
    this_id <- this_recording_info$recording_id
    this_deploy <- deploy_notes |> filter(recording_id %in% this_id)
    this_c1time <- strptime(this_deploy$cal1_time, "%d/%m/%Y %H:%M:%S")
    this_c3time <- strptime(this_deploy$cal3_time, "%d/%m/%Y %H:%M:%S")
    db <- dbConnect(SQLite(), path_raw_acc_db)
    thisID <- as.numeric(dbGetQuery(db, glue("SELECT id FROM recordings WHERE ", 
                                             "recording_id IS '{this_id}'")))
    this_c1_error <- this_recording_info$clock_error_1
    this_c3_error <- this_recording_info$clock_error_3
    c1_data <- dbGetQuery(db, glue(
            "SELECT * 
             FROM acc 
             WHERE datetime > '{this_c1time - 15}' 
                AND datetime <= '{this_c1time + 15}' 
                AND recording_id IS {thisID}")
    ) |> mutate(datetime = as.POSIXct(datetime, format = "%Y-%m-%d %H:%M:%OS", tz = "CET"))
    
    c3_data <- dbGetQuery(db, glue(
            "SELECT * 
             FROM acc 
             WHERE datetime > '{this_c3time - 15}' 
                AND datetime <= '{this_c3time + 15}' 
                AND recording_id IS {thisID}")
    ) |> mutate(datetime = as.POSIXct(datetime, format = "%Y-%m-%d %H:%M:%OS", tz = "CET"))
    dbDisconnect(db)

    c1_data |>
        ggplot(aes(x = datetime, y = accZ)) +
        geom_point() +
        geom_vline(aes(xintercept = this_c1time + this_c1_error), 
                   size = 1, alpha = 0.5) +
        scale_x_datetime(labels = scales::date_format("%Y-%m-%d %H:%M:%OS2", tz = "CET")) +
        labs(title = glue("i={i} id={this_id} calibration point 1")) +
    (c3_data |>
        ggplot(aes(x = datetime, y = accZ)) +
        geom_point() +
        geom_vline(aes(xintercept = this_c3time + this_c3_error ), 
                   size = 1, alpha = 0.5) +
        scale_x_datetime(labels = scales::date_format("%Y-%m-%d %H:%M:%OS2", tz = "CET")) +
        labs(title = "calibration point 3"))
}
```

Check drift rate & intercept calculations
```{r Check drift rate & intercept calculations}
any_problems <- FALSE
for(i in 1:length(unique(beh_obs_data$recording_id))){
    cat(glue("{i}."))
    thisid <- unique(beh_obs_data$recording_id)[i]
    this_deploy <- deploy_notes |> filter(recording_id %in% thisid)
    this_rec_info <- fread(path_recording_info) |> filter(recording_id %in% thisid)
    
    this_recording_origin_time <- toposix_dmyhms(this_deploy$start_dt)
    this_c1time <- toposix_dmyhms(this_deploy$cal1_time)
    this_c3time <- toposix_dmyhms(this_deploy$cal3_time)
    this_c1_error <- this_rec_info$clock_error_1
    this_c3_error <- this_rec_info$clock_error_3
    this_drift_rate <- this_rec_info$drift_rate
    this_drift_yintercept <- this_rec_info$drift_yintercept
    
    recalculated_drift_rate <- (this_c3_error - this_c1_error) / 
        as.numeric(difftime(this_c3time, this_c1time, units = "days"))
    recalculated_drift_yint <- as.numeric(difftime(this_c1time, this_recording_origin_time, units = "days")) * 
        recalculated_drift_rate + this_c1_error
    # small tolerance due to rounding dts
    if(!all(abs(this_drift_rate - recalculated_drift_rate) < 0.01, 
            abs(this_drift_yintercept - recalculated_drift_yint) < 0.01)){
        print(glue("i={i}, {thisid} drift rate or y intercept calcualtion did not match"))
        any_problems <- TRUE
    }
}
if(!any_problems) print("All OK")
```

Check conversion of behaviours real time to accelerometer time
```{r Check conversion of behaviours real time to accelerometer time}
# Pick a few times, reconvert behaviour times to accelerometer times. Check
#   that the notated time matches the expected time
any_problems <- FALSE
for(i in 1:length(unique(beh_obs_data$recording_id))){
    cat(glue("{i}."))
    thisid <- unique(beh_obs_data$recording_id)[i]
    this_rec_info <- fread(path_recording_info, tz = "") |> 
        filter(recording_id %in% thisid) |>
        mutate(deploy_time_start = toposix_dmyhms(deploy_time_start),
               deploy_time_end = toposix_dmyhms(deploy_time_end),
               acc_first_dt = toposix_ymdhms(acc_first_dt))
    behs_this_id <- beh_obs_data |> filter(recording_id == thisid)
    
    # rows <- seq(1, nrow(behs_this_id), 50)
    
    time_elapsed <- as.numeric(difftime(behs_this_id$beh_start_real,
                                        this_rec_info$deploy_time_start, 
                                        units = "days"))
    behs_this_id$start_acc_recalculated <- 
        behs_this_id$beh_start_real + 
        this_rec_info$drift_rate * time_elapsed + 
        this_rec_info$drift_yintercept
    behs_this_id$stop_acc_recalculated <- 
        behs_this_id$beh_stop_real + 
        this_rec_info$drift_rate * time_elapsed + 
        this_rec_info$drift_yintercept
    
    starttime_diff <- 
        abs(as.numeric(difftime(behs_this_id$start_acc_recalculated, 
                                behs_this_id$beh_start_acc, "secs")))
    endtime_diff <- 
        abs(as.numeric(difftime(behs_this_id$stop_acc_recalculated,
                                behs_this_id$beh_stop_acc, "secs")))
    start_match <- starttime_diff < 0.01
    end_match <- endtime_diff < 0.01

    if (!all(start_match & end_match)){
        print(glue("\n{thisid} behaviour real to acc time adjustments mismatched, ",
                   "{sum(!start_match)} start problems, {sum(!end_match)} end problems"))
        print(behs_this_id[!start_match|!end_match,] |> select(matches("start_acc|stop_acc")))
        any_problems <- TRUE
    }
}
if (!any_problems) print("All OK")
```

Check behaviour observation segmentation
```{r Check behaviour observation segmentation}
# Behaviour observations were not continuous and were divided into segments
# Checks for segments:
#       Is the full segment indeed in the behaviour file? 
#       Does the end of the segment truly correspond to a gap in the behaviour file? 
#       Are there unnecessary small gaps in behaviour scoring that might be able
#          to be filled in?

# Load behaviour data but add a couple of columns and split it by id
any_problems <- FALSE
temp <- beh_obs_data
beh_by_id <- split(temp, temp$recording_id)
beh_by_id <- map(beh_by_id, function(x){
    x <- x |> arrange(beh_start_real)
    
    x$elapsed_before_me <- 
        c(99999, as.numeric(difftime(x$beh_start_real[-1],
                                  x$beh_stop_real[-nrow(x)],
                                  units = "secs")))
    x$gap_before_me <- x$elapsed_before_me > 0.5
    x$same_beh_as_prev_row <- 
        c(FALSE, x$behaviour[-1] == x$behaviour[-length(x$behaviour)])

    return(x)})

# Check segments (top-down)
cat("\n---------------------Segment contiguity check-------------------------")
# -- No gaps seen in the middle of a segment? --
for (i in 1:nrow(segment_summary)){
    cat(glue("{i}."))
    thisseg <- segment_summary[i,]
    thisid <- thisseg$recording_id
    behs_thisid <- beh_by_id[[thisid]] |> arrange(beh_start_real)
    # Find gaps in the behaviour scoring sequence (max gap 0.5 is hard coded for now)
    gapsinthisseg <- behs_thisid |> filter(beh_start_real > thisseg$seg_start_real &
                               beh_stop_real < thisseg$seg_stop_real) |> pull(gap_before_me)
    if(any(gapsinthisseg)){
        print(glue("{this_id} segment {thisseg$segment_id} had gaps within it"))
        any_problems <- TRUE
    }
}
if (!any_problems){
    cat("\nAll ok.")
} 
any_problems <- FALSE

# -- Gaps between segments should have no 'floating' behavioral data --
cat("\n--------------------Floating behaviours check-----------------------------")
for (i in 1:length(unique(beh_obs_data$recording_id))){
    cat(glue("{i}."))
    thisid <- unique(beh_obs_data$recording_id)[i]
    behs_thisid <- beh_by_id[[thisid]]
    segs_thisid <- segment_summary |> filter(recording_id == thisid) |> arrange(seg_start_real)
    if (nrow(segs_thisid) == 1) next
    segment_gaps <-
        data.frame(gap_start = segs_thisid$seg_stop_real[-nrow(segs_thisid)],
                   gap_stop = segs_thisid$seg_start_real[-1])
    # go over each gap
    for(i in 1:nrow(segment_gaps)){
        # Check if there are any behaviours in this time
        floating_behaviours <- 
            behs_thisid |> filter(beh_start_real > segment_gaps$gap_start[i] & 
                                   beh_stop_real < segment_gaps$gap_stop[i])
        if (nrow(floating_behaviours) > 0){
            print(glue("{thisid} segment {segs_thisid$segment_id[i]}, i={i} had an" , 
                       " adjacent floating behaviour"))
            any_problems <- TRUE
        }
    }
}
if (!any_problems){
    cat("\nAll ok.")
} 
any_problems <- FALSE

cat("\n---------------------Check segment start times-----------------------------")
# Go over all ids and get info
for (i in 1:length(unique(beh_obs_data$recording_id))){
    cat(glue("{i}."))
    thisid <- unique(beh_obs_data$recording_id)[i]
    # -- Behaviours with a gap before them should be the start of a new segment --
    behs_thisid <- beh_by_id[[thisid]]
    segs_thisid <- segment_summary |> filter(recording_id == thisid) |> arrange(seg_start_real)
    
    seg_starts_recalculated <- behs_thisid[behs_thisid$gap_before_me, 'beh_start_real']
    seg_starts_from_outputfile <- segs_thisid$seg_start_real
    a <- prtime(seg_starts_recalculated) 
    b <- prtime(seg_starts_from_outputfile)
    mismatch <- !all(a == b)
    if(any(mismatch)){
        print(glue("{this_id} had unverified segment times issues: segmented ", 
                   "behaviour times do not seem to match up with behaviour gaps"))
        any_problems <- TRUE
    }
}
if (!any_problems) cat("\nAll ok.")

## Design decision was made to allow successive identical behaviours
# print("---------------Check for successive identical behaviours---------------")
# # Check if there are two behaviours in a row with the same label. These should be
# combined into a single behaviour
# for (i in 1:length(beh_by_id)){
#     cat(glue("{i}."))
#     thisid <- unique(beh_obs_data$recording_id)[i]
#     behs_thisid <- beh_by_id[[thisid]]
#     
#     repeated_behaviours <- behs_thisid$same_beh_as_prev_row & !behs_thisid$gap_before_me
# 
#     if(any(repeated_behaviours)){
#         cat("\n")
#         print(glue("i={i} {thisid} had repeated subsequent behaviours\n"))
#     }
# }
```

Check label assignments
```{r Check label assignments (bottom-up)}
# Pick out random behaviours from beh_acc. Get the start and stop time for this 
# beh and check that the labelled data file has the correct behaviour labels
# between these times.
any_problems <- FALSE
nsamples <- 300
random_behs <- beh_obs_data[sample(1:nrow(beh_obs_data), nsamples),]
random_behs$assigned_seg <- NA

# First, get the data segment corresponding to the randomly chosen behaviour. 
# Incidentally, check that there is a segment whose time overlaps with this
# behaviour, and only 1
for (i in 1:nrow(random_behs)){
    cat(glue("{i}."))
    thisbeh <- random_behs[i,]
    assigned_seg <- segment_summary |> 
        filter(recording_id == thisbeh$recording_id) |> 
        filter(seg_start_real <= thisbeh$beh_start_real) |>
        filter(seg_stop_real  >= thisbeh$beh_stop_real) |>
        pull(segment_id)
    
    if(length(assigned_seg) != 1){
        print(glue("Behaviour {thisbeh$beh_event_id}: beh times overlaps with ",
                   "multiple or zero segments"))
            any_problems <- TRUE
    }
    random_behs$assigned_seg[i] <- assigned_seg
}

# Then, fetch the labelled data segment that this behaviour should be present in. 
# Check that the correct part of the data file labelled. 
# note: caching used so the same segment isn't loaded multiple times
random_behs <- random_behs |> arrange(assigned_seg)
cached_seg <- -1 # initialise with placeholder number
for (i in 1:nrow(random_behs)){
    # for each behaviour
    cat(glue("{i}."))
    thisbeh <- random_behs[i,]

    # Find which segmented file this behaviour should be represented in
    thisid <- thisbeh$recording_id
    thisseg <- thisbeh$assigned_seg
    
    # Get the datafile generated for this segment, if it's not already loaded
    if(thisseg != cached_seg){
        thisseg_data <- 
            fread(file.path(processed_data_dir, glue("{thisid}_s{thisseg}.csv")),
                  tz = "") |> 
            mutate(datetime = toposix_ymdhms(datetime))
        cached_seg <- thisseg
    }

    # Check the expected rows of the file, and see if any neighbouring behaviours
    # are intruding, or perhaps if the whole thing is outright incorrectly labelled
    matching_data <- thisseg_data |> 
        filter(datetime > thisbeh$beh_start_acc & datetime < thisbeh$beh_stop_acc)
    
    if(length(unique(matching_data$behaviour)) > 1){
        print(glue("\ni={i}Behaviour {thisbeh$beh_event_id} segment {thisseg}: matching",
                   " part of annotated data was not pure"))
            any_problems <- TRUE
    } else if(thisbeh$behaviour != unique(matching_data$behaviour)){
        print(glue("\ni={i}Behaviour {thisbeh$beh_event_id} segment {thisseg}: matching",
                   " annotated data had wrong label"))
            any_problems <- TRUE
    }
    
    data_subsequent_rows <- thisseg_data |> 
        filter(datetime > thisbeh$beh_stop_acc & datetime < thisbeh$beh_stop_acc + 0.25)
    data_prior_rows <- thisseg_data |> 
        filter(datetime > thisbeh$beh_start_acc - 0.25 & datetime < thisbeh$beh_start_acc)
    
    subsequent_bleeding <- data_subsequent_rows$behaviour[1] %in% matching_data$behaviour
    prior_bleeding <- last(data_prior_rows$behaviour) %in% matching_data$behaviour
    # If any of the target behaviour appears in the nearby ~quarter of second,
    # there may be a labeling problem. Check the beahiviour scoring and see if
    # the next/previous behaviour is truly a repetition or not. These repetitions
    # came about sometimes due to changing behaviour labels earlier in the process.
    if(subsequent_bleeding){
        # It's not bleeding if the next event is truly the same behaviour
        next_event <- thisbeh$beh_event_id+1
        if (!thisbeh$behaviour == beh_obs_data[beh_obs_data$beh_event_id == next_event, "behaviour"]){
            print(glue("\ni={i} Behaviour {thisbeh$beh_event_id} segment {thisseg}: bled ",
                       " into from subsequent behaviour event"))
            any_problems <- TRUE
        }
    }
    if(prior_bleeding){
        # It's not bleeding if the prev event is truly the same behaviour
        prev_event <- thisbeh$beh_event_id-1
        if (!thisbeh$behaviour == beh_obs_data[beh_obs_data$beh_event_id == prev_event, "behaviour"]){
            print(glue("\ni={i} Behaviour {thisbeh$beh_event_id} segment {thisseg}: bled ",
                       "into by prior behaviour event"))
            any_problems <- TRUE
        }
    }
}
if (!any_problems) print("All OK")

```

Check 6O scaling factors
```{r Check 6-O scaling factors pt 1}
# Recalculate 6-O factors and check that it matches expected values
any_problems <- FALSE
info_6O <- fread(path_6O_calib_info, tz = "")
recording_info <- fread(path_recording_info, tz = "")
# Select wanted columns
info_6O$start_dt <- paste(info_6O$start_date, info_6O$start_time)
info_6O <- info_6O[,.(logger_id, filename, start_dt, z1_cal_dt, z2_cal_dt, 
                      y1_cal_dt, y2_cal_dt, x1_cal_dt, x2_cal_dt)]

# Convert to datetimes
cols <- names(info_6O)[grepl("dt", names(info_6O))]
info_6O[, (cols) := lapply(.SD, function(x)dmy_hms(x, tz = "CET")), .SDcols = cols]

get_norm <- function(datatable){
    sqrt(datatable$accX^2 + datatable$accY^2 + datatable$accZ^2)
}

plots <- list()

# For each recording, recalculate and check the 6O scaling factors. Also 
# apply scaling factor to calibration data and check it results in acc ~= 1
for (i in 1:nrow(recording_info)){
    thisrecinfo <- recording_info[i,]
    this6Oinfo <- info_6O[info_6O$logger_id == thisrecinfo$logger_id,]
    thisx1dt <- this6Oinfo$x1_cal_dt
    thisx2dt <- this6Oinfo$x2_cal_dt
    thisy1dt <- this6Oinfo$y1_cal_dt
    thisy2dt <- this6Oinfo$y2_cal_dt
    thisz1dt <- this6Oinfo$z1_cal_dt
    thisz2dt <- this6Oinfo$z2_cal_dt
    
    thisdat <- fread(file.path(dir_acc_calib_data, this6Oinfo$filename))
    thisdat$dt <- dmy_hms(paste(thisdat$Date, thisdat$Time), tz = "CET")
    thisdat <- thisdat |> filter(dt > min(thisx1dt, thisx2dt, thisy1dt, 
                                           thisy2dt, thisz1dt, thisz2dt) - 5)
    thisdat <- thisdat |> filter(dt < max(thisx1dt, thisx2dt, thisy1dt, 
                                           thisy2dt, thisz1dt, thisz2dt) + 5)
    x1 <- thisdat[dt > thisx1dt & dt < thisx1dt + 5] |> get_norm() |> median()
    x2 <- thisdat[dt > thisx2dt & dt < thisx2dt + 5] |> get_norm() |> median()
    y1 <- thisdat[dt > thisy1dt & dt < thisy1dt + 5] |> get_norm() |> median()
    y2 <- thisdat[dt > thisy2dt & dt < thisy2dt + 5] |> get_norm() |> median()
    z1 <- thisdat[dt > thisz1dt & dt < thisz1dt + 5] |> get_norm() |> median()  
    z2 <- thisdat[dt > thisz2dt & dt < thisz2dt + 5] |> get_norm() |> median()

    if (((1 / mean(c(x1, x2))) - thisrecinfo$scale_x_6O) > 0.01){
        print(glue("recording {thisrecinfo$recording_id} X scaling didn't match. ",
                   "orig {round(thisrecinfo$scale_x_6O, 3)}, ",
                   "recalculated{round(1/mean(c(x1,x2)),3)}"))
        any_problems <- TRUE
    }
    if (((1 / mean(c(y1, y2))) - thisrecinfo$scale_y_6O) > 0.01){
        print(glue("recording {thisrecinfo$recording_id} Y scaling didn't match. ",
                   "orig {round(thisrecinfo$scale_y_6O, 3)}, ",
                   "recalculated {round(1/mean(c(y1,y2)),3)}"))
        any_problems <- TRUE
    }
    if (((1 / mean(c(z1, z2))) - thisrecinfo$scale_z_6O) > 0.01){
        print(glue("recording {thisrecinfo$recording_id} Z scaling didn't match. ",
                   "orig {round(thisrecinfo$scale_z_6O, 3)}, ",
                   "recalculated {round(1/mean(c(z1,z2)),3)}"))
        any_problems <- TRUE
    }
    # Apply scaling factor to data and see if it equals 1.
    back_dat <- thisdat
    back_dat$accX <- back_dat$accX * thisrecinfo$scale_x_6O
    back_dat$accY <- back_dat$accY * thisrecinfo$scale_y_6O
    back_dat$accZ <- back_dat$accZ * thisrecinfo$scale_z_6O
    x1 <- back_dat[dt > thisx1dt & dt < thisx1dt + 5] |> get_norm() |> median()
    x2 <- back_dat[dt > thisx2dt & dt < thisx2dt + 5] |> get_norm() |> median()
    y1 <- back_dat[dt > thisy1dt & dt < thisy1dt + 5] |> get_norm() |> median()
    y2 <- back_dat[dt > thisy2dt & dt < thisy2dt + 5] |> get_norm() |> median()
    z1 <- back_dat[dt > thisz1dt & dt < thisz1dt + 5] |> get_norm() |> median()
    z2 <- back_dat[dt > thisz2dt & dt < thisz2dt + 5] |> get_norm() |> median()
    if(abs(x1 - 1) > 0.01|abs(x2 - 1) > 0.01|abs(y1 - 1) > 0.01|abs(y2 - 1) > 0.01|
       abs(z1 - 1) > 0.01|abs(z2 - 1) > 0.01){
        print(glue("recording {thisrecinfo$recording_id} applying scaling ",
                   "factor to data does not yield g = 1. Double-check vals:\n", 
                   "     x1 : {round(x1,2)}    x2 : {round(x2,2)}  ",
                   "   y1 : {round(y1,2)}    y2 : {round(y2,2)}  ",
                   "   z1 : {round(z1,2)}    z2 : {round(z2,2)}"))
    }
    rects <- data.frame(var = c('accX', 'accX', 'accY','accY','accZ','accZ'),
                        xmin = c(thisx1dt, thisx2dt, thisy1dt, thisy2dt, 
                                 thisz1dt, thisz2dt), ymax = Inf, ymin = -Inf)
    rects$xmax = rects$xmin + 5
    points <- data.frame(var = c('accX', 'accX', 'accY','accY','accZ','accZ'),
                        x = c(thisx1dt, thisx2dt, thisy1dt, thisy2dt, 
                                 thisz1dt, thisz2dt), 
                        y = c(-1/ thisrecinfo$scale_x_6O, 1/ thisrecinfo$scale_x_6O,
                              -1/ thisrecinfo$scale_y_6O, 1/ thisrecinfo$scale_y_6O,
                              -1/ thisrecinfo$scale_z_6O, 1/ thisrecinfo$scale_z_6O))
    points$x = points$x + 2.5
    plots[[i]] <- thisdat |> 
        pivot_longer(cols = c(accX, accY, accZ), 
                     names_to = "var", values_to = "val") |>
        ggplot() + 
        geom_line(aes(x = dt, y = val)) + 
        facet_wrap(~var, ncol = 1, scales = "free") + 
        geom_rect(data = rects, aes(xmin = xmin, xmax = xmax, ymin = ymin, 
                                    ymax = ymax, col = var), fill = NA) +
        geom_point(data = points, aes(x = x, y = y, col = var)) +
        coord_cartesian(ylim = c(-2.5, 2.5))
}
```

```{r Show plots for 6O scaling}
plots
```

```{r Check 6-O scaling factors pt 2}
# Check that the 6-O scaled data results in static acceleration values 
# that average to  g = 1

for (i in 1:nrow(recording_info)){
    thisinfo <- recording_info[i,]
    thisinfo$cal1_time <- toposix_dmyhms(thisinfo$cal1_time)
    thisinfo$cal3_time <- toposix_dmyhms(thisinfo$cal3_time)
    thisinfo$deploy_time_start <- toposix_dmyhms(thisinfo$deploy_time_start)
    setDT(thisinfo)
    cols <- names(thisinfo)[grepl("dt|time", names(thisinfo))]
    thisinfo[, (cols) := lapply(.SD, function(x) toposix_dmyhms(x)), .SDcols = cols]
    # Find a time when the logger was being calibrated before bird deployment,
    # it should have been nice and stationary just before being calibrated
    calib1time <- adjust_time(target_time = thisinfo$cal1_time[1], 
                              origin = thisinfo$deploy_time_start,
                              dr = thisinfo$drift_rate,
                              init_error = thisinfo$drift_yintercept)
    calib3time <- adjust_time(target_time = thisinfo$cal3_time[1], 
                              origin = thisinfo$deploy_time_start,
                              dr = thisinfo$drift_rate,
                              init_error = thisinfo$drift_yintercept)

    # Get 5 hour buffer around this time
    calib1_data <- get_raw_acc_segment(gsub(".csv", "", thisinfo$recording_id),
                        segment_start = calib1time - 30, 
                        duration_secs = 15,
                        path_db = path_raw_acc_db)
    calib3_data <- get_raw_acc_segment(gsub(".csv", "", thisinfo$recording_id),
                        segment_start = calib3time - 30, 
                        duration_secs = 15,
                        path_db = path_raw_acc_db)
    calib1_data$accX <- calib1_data$accX * thisinfo$scale_x_6O
    calib1_data$accY <- calib1_data$accY * thisinfo$scale_y_6O
    calib1_data$accZ <- calib1_data$accZ * thisinfo$scale_z_6O
    calib1_data$norm <- sqrt(calib1_data$accX^2 + calib1_data$accY^2 + calib1_data$accZ^2)
    med_1 <- median(calib1_data$norm)
    
    calib3_data$accX <- calib3_data$accX * thisinfo$scale_x_6O
    calib3_data$accY <- calib3_data$accY * thisinfo$scale_y_6O
    calib3_data$accZ <- calib3_data$accZ * thisinfo$scale_z_6O
    calib3_data$norm <- sqrt(calib3_data$accX^2 + calib3_data$accY^2 + calib3_data$accZ^2)
    med_3 <- median(calib3_data$norm)
    if (med_1 > 1.05 | med_1 < 0.95 | med_3 > 1.05 | med_3 < 0.95){
        cat("PROBLEM?-")
        any_problems <- TRUE
    } else {
        cat("---------")
    }
    print(glue("Recording {stri_pad_right(thisinfo$recording_id, 14, ' ')} : ",
       "g for calibration 1;3 (should be near 1) = {sprintf('%.2f', round(med_1,2))};",
       "{sprintf('%.2f',round(med_3,2))}"))
}

```

Check deployment bias (median adjustment)
```{r Check deployment bias values}
# Median scaling factors: calculate a rougher median scaling 
#   factors (e.g. with the subsample of data rather than full recording) 
#   and check they are similar enough to the originally calculated scaling factor

print(glue("Recalculating median scaling factors using an estimate. Recalculated",
           "values should be in the same ballpark"))
for (i in 1:nrow(recording_info)){
    thisinfo <- recording_info[i,]
    firstbeh <- beh_obs_data |> filter(recording_id == thisinfo$recording_id) |>
        arrange(beh_start_real)
    firstbehtime <- firstbeh$beh_start_acc[1]
    # Get some  data from middle of their recording
    calib1_data <- get_raw_acc_segment(gsub(".csv", "", thisinfo$recording_id),
                        segment_start = firstbehtime - 60 * 60, # 1 hour before first beh
                        duration_secs = 60 * 60 * 5, # 5 hours of behaviour
                        path_db = path_raw_acc_db)
    
    # Do 6O correction
    calib1_data$accX <- calib1_data$accX * thisinfo$scale_x_6O
    calib1_data$accY <- calib1_data$accY * thisinfo$scale_y_6O
    calib1_data$accZ <- calib1_data$accZ * thisinfo$scale_z_6O
    # See how recalculated medians compare to the original processed value
    xdiff <- abs(median(calib1_data$accX) - thisinfo$scale_x_deploybias)
    ydiff <- abs(median(calib1_data$accY) - thisinfo$scale_y_deploybias)
    zdiff <- abs(median(calib1_data$accZ) - thisinfo$scale_z_deploybias)
    
    if (xdiff > 0.05 | ydiff > 0.05 | zdiff > 0.05){
        cat("PROBLEM?-")
        any_problems <- TRUE
    } else{
        cat("---------")
    }
    print(glue("Recording {stri_pad_right(thisinfo$recording_id, 14, ' ')} ",
          "median scales (new/orig). ", 
          "X: {sprintf('%.2f', round(median(calib1_data$accX),2))}/",
          "{sprintf('%.2f', round(thisinfo$scale_x_deploybias,2))}",
          " Y: {sprintf('%.2f', round(median(calib1_data$accY),2))}/",
          "{sprintf('%.2f', round(thisinfo$scale_y_deploybias,2))}",
          " Z: {sprintf('%.2f', round(median(calib1_data$accZ),2))}/",
          "{sprintf('%.2f', round(thisinfo$scale_z_deploybias,2))}"))
}

if (!any_problems) print("All OK")

```

Check for NA values?
```{r}
# Not implemented
```

Check for duplicated timestamps?
```{r}
# Not implemented
```
